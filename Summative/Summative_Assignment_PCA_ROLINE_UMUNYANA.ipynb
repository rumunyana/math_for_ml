{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyATLU4z1cYj"
      },
      "source": [
        "\n",
        "\n",
        "<center>\n",
        "    <img src=\"https://miro.medium.com/v2/resize:fit:300/1*mgncZaKaVx9U6OCQu_m8Bg.jpeg\">\n",
        "</center>\n",
        "\n",
        "\n",
        "\n",
        "The goal of PCA is to extract information while reducing the number of features\n",
        "from a dataset by identifying which existing features relate to another. The crux of the algorithm is trying to determine the relationship between existing features, called principal components, and then quantifying how relevant these principal components are. The principal components are used to transform the high dimensional data to a lower dimensional data while preserving as much information. For a principal component to be relevant, it needs to capture information about the features. We can determine the relationships between features using covariance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UTntK0eUNimH"
      },
      "outputs": [],
      "source": [
        "#import necessary package\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWaiAdz8PyKp"
      },
      "outputs": [],
      "source": [
        "### Representing the Data\n",
        "# data has shape (n, d)\n",
        "data = np.array([\n",
        "    [   1,   2,  -1,   4,  10],\n",
        "    [   3,  -3,  -3,  12, -15],\n",
        "    [   2,   1,  -2,   4,   5],\n",
        "    [   5,   1,  -5,  10,   5],\n",
        "    [   2,   3,  -3,   5,  12],\n",
        "    [   4,   0,  -3,  16,   2],\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2U2_Q5ebos3"
      },
      "source": [
        "### Step 1: Standardize the Data along the Features\n",
        "\n",
        "![image.png](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQLxe5VYCBsaZddkkTZlCY24Yov4JJD4-ArTA&usqp=CAU)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Explain why we need to handle the data on the same scale.\n",
        "\n",
        "Data must be standardized along the features before applying PCA to ensure each variable contributes equally to the analysis, avoiding bias where variables with larger scales dominate, and to aid in the convergence of algorithms by providing a consistent scale for comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JF3eGB7FRC0A",
        "outputId": "6823f9b1-9e2c-43bd-9c0c-1032082c94ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[-1.36438208  0.70710678  1.5109662  -0.99186978  0.77802924]\n",
            " [ 0.12403473 -1.94454365 -0.13736056  0.77145428 -2.06841919]\n",
            " [-0.62017367  0.1767767   0.68680282 -0.99186978  0.20873955]\n",
            " [ 1.61245155  0.1767767  -1.78568733  0.33062326  0.20873955]\n",
            " [-0.62017367  1.23743687 -0.13736056 -0.77145428  1.00574511]\n",
            " [ 0.86824314 -0.35355339 -0.13736056  1.65311631 -0.13283426]]\n"
          ]
        }
      ],
      "source": [
        "mean = np.mean(data, axis=0)\n",
        "\n",
        "data.std = np.std(data, axis=0)\n",
        "\n",
        "# Standardize the data\n",
        "\n",
        "standardized_data = (data - mean) / data.std\n",
        "\n",
        "print(standardized_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rzoiQ7fMk_d"
      },
      "source": [
        "![cov matrix.webp](https://dmitry.ai/uploads/default/original/1X/9bd2851674ebb55e404cc3ff5e2ffe65b42ff460.png)\n",
        "\n",
        "We use the pair - wise covariance of the different features to determine how they relate to each other. With these covariances, our goal is to group / cluster based on similar patterns. Intuitively, we can relate features if they have similar covariances with other features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuhux3UEcBgw"
      },
      "source": [
        "### Step 2: Calculate the Covariance Matrix\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qn8oujZlK9YR",
        "outputId": "08f56eea-4404-4de5-b04c-b81a63afff33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 1.2        -0.42098785 -1.0835838   0.90219291 -0.37000528]\n",
            " [-0.42098785  1.2         0.20397003 -0.77149364  1.18751836]\n",
            " [-1.0835838   0.20397003  1.2        -0.59947269  0.22208218]\n",
            " [ 0.90219291 -0.77149364 -0.59947269  1.2        -0.70017993]\n",
            " [-0.37000528  1.18751836  0.22208218 -0.70017993  1.2       ]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "cov_matrix = np.cov(standardized_data, ddof = 1, rowvar = False)\n",
        "\n",
        "print(cov_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXNcG4AFcT08"
      },
      "source": [
        "### Step 3: Eigendecomposition on the Covariance Matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmGlQ47tRO5w",
        "outputId": "b3488230-97d2-43aa-ebfd-e0ea78288807"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Eigenvalues: \n",
            "[3.80985761e+00 1.73655615e+00 4.94531029e-02 4.74189469e-05\n",
            " 4.04085720e-01]\n",
            "\n",
            "Eigenvectors: \n",
            "[[-0.4640131   0.45182808 -0.70733581  0.28128049 -0.03317471]\n",
            " [ 0.45019005  0.48800851  0.29051532  0.6706731  -0.15803498]\n",
            " [ 0.37929082 -0.55665017 -0.48462321  0.24186072 -0.5029143 ]\n",
            " [-0.4976889   0.03162214  0.36999674 -0.03373724 -0.78311558]\n",
            " [ 0.43642295  0.49682965 -0.20861365 -0.64143906 -0.32822489]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
        "\n",
        "print(f\"\\nEigenvalues: \\n{eigenvalues}\")\n",
        "print(f\"\\nEigenvectors: \\n{eigenvectors}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pWho88fcbJA"
      },
      "source": [
        "### Step 4: Sort the Principal Components\n",
        "# np.argsort can only provide lowest to highest; use [::-1] to reverse the list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_znKtzdrTmMg",
        "outputId": "823452d8-5e96-4ffa-a339-b35e944d8ea1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the order of importance is :\n",
            " [0 1 4 2 3]\n",
            "\n",
            "\n",
            " sorted eigen values:\n",
            "[3.80985761e+00 1.73655615e+00 4.04085720e-01 4.94531029e-02\n",
            " 4.74189469e-05]\n",
            "\n",
            "\n",
            " The sorted eigen vector matrix is: \n",
            " [[-0.4640131   0.45182808 -0.03317471 -0.70733581  0.28128049]\n",
            " [ 0.45019005  0.48800851 -0.15803498  0.29051532  0.6706731 ]\n",
            " [ 0.37929082 -0.55665017 -0.5029143  -0.48462321  0.24186072]\n",
            " [-0.4976889   0.03162214 -0.78311558  0.36999674 -0.03373724]\n",
            " [ 0.43642295  0.49682965 -0.32822489 -0.20861365 -0.64143906]]\n"
          ]
        }
      ],
      "source": [
        "# np.argsort can only provide lowest to highest; use [::-1] to reverse the list\n",
        "\n",
        "order_of_importance = np.argsort(eigenvalues)[::-1]\n",
        "print ( 'the order of importance is :\\n {}'.format(order_of_importance))\n",
        "\n",
        "# utilize the sort order to sort eigenvalues and eigenvectors\n",
        "sorted_eigenvalues = eigenvalues[order_of_importance]\n",
        "\n",
        "\n",
        "print('\\n\\n sorted eigen values:\\n{}'.format(sorted_eigenvalues))\n",
        "sorted_eigenvectors = eigenvectors[:,order_of_importance] # sort the columns\n",
        "print('\\n\\n The sorted eigen vector matrix is: \\n {}'.format(sorted_eigenvectors))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1nILNGxpTJB"
      },
      "source": [
        "Question:\n",
        "\n",
        "1. Why do we order eigen values and eigen vectors?\n",
        "\n",
        "Eigenvalues and eigenvectors are ordered to identify and prioritize the principal components that capture the most significant variance in the data.\n",
        "\n",
        "2. Is it true we would consider the lowest eigen value compared to the highest? Defend your answer\n",
        "\n",
        "Yes , we do prioritize them because they represent the principal components with the most variance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWqFGNeNvgEB"
      },
      "source": [
        "You want to see what percentage of information each eigen value holds. You would have print out the percentage of each eigen value using the formula\n",
        "\n",
        "\n",
        "\n",
        "> (sorted eigen values / sum of all sorted eigen values) * 100\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRMHrffrVOXR",
        "outputId": "de769063-84e5-49e0-ca44-b67433fac418"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['0.63%', '0.29%', '0.07%', '0.01%', '0.00%']\n"
          ]
        }
      ],
      "source": [
        "# use sorted_eigenvalues to ensure the explained variances correspond to the eigenvectors\n",
        "\n",
        "#TO DO: Insert code here\n",
        "explained_variance = sorted_eigenvalues / np.sum(sorted_eigenvalues)\n",
        "explained_variance =[\"{:.2f}%\".format(value) for value in explained_variance]\n",
        "print( explained_variance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB7H4InbfKx5"
      },
      "source": [
        "#Initialize the number of Principle components then perfrom matrix multiplication with the variable K example k = 3 for 3 priciple components\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> The reulting matrix (with reduced data) = standardized data * vector with columns k\n",
        "\n",
        "See expected output for k = 2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-Rnyq6QVTiz"
      },
      "outputs": [],
      "source": [
        "k = 2 # select the number of principal components\n",
        "\n",
        "reduced_data_vectors = sorted_eigenvectors[:, order_of_importance[:k]]\n",
        "reduced_data = np.matmul(standardized_data,reduced_data_vectors) # transform the original data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxxBcgQMXe1h",
        "outputId": "76eb09ad-ace9-4d15-bb2b-59b67d8092fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 1.07127878 -0.6983307 ]\n",
            " [-3.49014682 -0.59870297]\n",
            " [ 0.24003422 -0.48244534]\n",
            " [-0.14516166 -1.61189378]\n",
            " [ 1.34022572 -1.07434063]\n",
            " [-0.96573453 -2.23341502]]\n"
          ]
        }
      ],
      "source": [
        "print(reduced_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNEqS6cuaMSY",
        "outputId": "78e448df-45c8-45d7-dbfb-6bf810f2a486"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(6, 2)\n"
          ]
        }
      ],
      "source": [
        "print(reduced_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxQ8lTunauMQ"
      },
      "source": [
        "# *What are 2 positive effects and 2 negative effects of PCA\n",
        "\n",
        "Give 2 Benefits and 2 limitations\n",
        "Positive Effects of PCA:\n",
        "\n",
        "Dimensionality Reduction: PCA simplifies large datasets by reducing the number of variables to a smaller set of principal components, which still capture the essential patterns and significant trends. This leads to reduced computational requirements and facilitates the analysis and visualization of the data.\n",
        "\n",
        "Feature Extraction and Noise Reduction: By transforming correlated variables into a set of linearly uncorrelated components, PCA emphasizes variation and brings out strong patterns in the dataset. This helps in extracting the most informative features and reduces the noise, making the underlying structure of the data more visible.\n",
        "\n",
        "Negative Effects of PCA:\n",
        "\n",
        "Loss of Interpretability: The principal components generated by PCA are linear combinations of the original variables, and they usually do not have a natural or intuitive interpretation. This loss of interpretability can be a significant drawback when the understanding of the underlying processes is essential.\n",
        "\n",
        "Information Loss and Over-Simplification: While PCA is designed to retain the most variance in the data with fewer components, it can also lead to the loss of information, especially if important but less variable features are discarded. In cases where subtle details are critical, this over-simplification may lead to inadequate or misleading conclusions. Additionally, PCA is sensitive to the scale of the features, meaning that it can be heavily influenced by the presence of outliers, which may affect the stability of the results."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
